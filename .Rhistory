plot(model.pls, ncomp = 1, asp = 1, line = TRUE)
plot(model.pls, ncomp = 2, asp = 1, line = TRUE)
plot(model.pls, ncomp = 3, asp = 1, line = TRUE)
plot(model.pls, ncomp = 4, asp = 1, line = TRUE)
plot(model.pls, ncomp = 8, asp = 1, line = TRUE)
plot(RMSEP(model.pls),legendpos="topright")
model.pls <- plsr(lpsa ~ .,4, data = train, method = "oscorespls")
plot(model.pls, plottype = "scores", comps = 1:3)
beta.pls <- drop(coef(model.pls))
resid.pls <- drop(model.pls$resid)[,4]
rss.pls <- sum(resid.pls^2)/(67-4)
rss.ls
rss.backward
rss.forward
rss.ridge
rss.lasso
rss.pls
y.new <- calibrate$lpsa
pss.ls <- sum((y.new - predict(model.ls, calibrate[,1:8]))^2)
pss.backward <- sum((y.new - predict(model.backward, calibrate[,1:8]))^2)
pss.forward <- sum((y.new - predict(model.forward, calibrate[,1:8]))^2)
pss.ridge <- sum((y.new - beta.ridge[1] - as.matrix(calibrate[,1:8])%*%beta.ridge[2:9])^2)
pss.lasso <- sum((y.new - predict(model.lasso, as.matrix(calibrate[,1:8]), s=4, type="fit")$fit)^2)
pss.pls <- sum((y.new - drop(predict(model.pls, calibrate[,1:8], 4)))^2)
pss.ls
pss.backward
pss.forward
pss.ridge
pss.lasso
pss.pls
url <- "http://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data"
str(pcancer <- read.table(url, header=TRUE))
train <- pcancer[which(pcancer$train),1:9]
calibrate <- pcancer[-which(pcancer$train),1:9]
model.backward <- step(model.ls, direction="backward")
summary(model.backward)
rss.backward <- sum(model.backward$resid^2)/model.backward$df.residual
scope <- list(upper=~lcavol+lweight+age+lbph+svi+lcp+gleason+pgg45, lower=~.)
model.forward <- step(lm(lpsa ~ 1, data=train), scope, direction="forward")
rss.forward <- sum(model.forward$resid^2)/model.forward$df.residual
r2 <- list()
AICs <- list()
for(i in 1:8){
indexes <- combn(8,i)  #combn(n,r) produces a list of all combinations: n choose r
currentr2 <- NULL
currentAIC <- NULL
for(j in 1:dim(indexes)[2]){
temp.model <- lm(lpsa ~ ., data=train[,c(indexes[,j], 9)])
currentr2[j] <- summary(temp.model)$r.squared
currentAIC[j] <- AIC(temp.model)
}
r2[[i]] <- currentr2
AICs[[i]] <- currentAIC
}
compare <- function(set){
s <- length(set)
temp <- combn(8,s)
check <- NULL
for(i in 1:dim(temp)[2]){
check[i] <- all(temp[,i]==set)
}
return(which(check))
}
backward <- compare(c(1:6,8))
forward <- c(compare(1), compare(1:2), compare(c(1,2,5)), compare(c(1,2,4,5)))
r2.b <- c(r2[[7]][backward], r2[[8]])
r2.f <- c(r2[[1]][forward[1]], r2[[2]][forward[2]], r2[[3]][forward[3]], r2[[4]][forward[4]])
AICs.b <- c(AICs[[7]][backward], AICs[[8]])
AICs.f <- c(AICs[[1]][forward[1]], AICs[[2]][forward[2]], AICs[[3]][forward[3]], AICs[[4]][forward[4]])
layout(matrix(1:2, ncol=2))
plot(0, xlim=c(0,9), ylim=c(0,0.8), type="n", ylab=expression(r^2), main="Fitting criteria")
for(i in 1:8){
points(rep(i, length(r2[[i]])), r2[[i]], pch=21, bg="Grey")
}
points(7:8, r2.b, bg="Red", col="Red", pch=21, type="o")
points(1:4, r2.f, bg="Blue", col="Blue", pch=21, type="o")
plot(0, xlim=c(0,9), ylim=c(153,217), type="n", ylab="AIC", main="AIC")
for(i in 1:8){
points(rep(i, length(AICs[[i]])), AICs[[i]], pch=21, bg="Grey")
}
points(7:8, AICs.b, bg="Red", col="Red", pch=21, type="o")
points(1:4, AICs.f, bg="Blue", col="Blue", pch=21, type="o")
library(ggplot2)
library(car)
library(MASS)
dev.off()  #reset the graphics window
model.ridge <- lm.ridge(lpsa ~ ., data=train, lambda = seq(0,10,0.1))  #ridge regression is run for several values of lambda
str(model.ridge)
qplot(model.ridge$lambda,model.ridge$GCV )        # plots generalized cross validated error (GCV) vs the tuning parameter
which.min(model.ridge$GCV)   # identifies the value for lambda with the smallest associated GCV
lambda.ridge <- seq(0,10,0.1)[which.min(model.ridge$GCV)]   #save the value of optimal lambda for future use
coef(model.ridge)
colors <- rainbow(8)
matplot(seq(0,10,0.1), coef(model.ridge)[,-1], xlim=c(0,11), type="l",xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors, lty=1, lwd=2, main="Ridge coefficients")
abline(v=lambda.ridge, lty=2)
abline(h=0, lty=2)
text(rep(10, 9), coef(model.ridge)[length(seq(0,10,0.1)),-1], colnames(train)[-9], pos=4, col=colors)
beta.ridge <- coef(model.ridge)[which.min(model.ridge$GCV),]
resid.ridge <- train$lpsa - beta.ridge[1] - as.matrix(train[,1:8])%*%beta.ridge[2:9]
d <- svd(as.matrix(train[,1:8]))$d
df <- 67 - sum(d^2/(lambda.ridge+d^2))
rss.ridge <- sum(resid.ridge^2)/df     #compute the mean RSS for ridge regression ()
library(lars)
y <- as.numeric(train[,9])      #target variable
x <- as.matrix(train[,1:8])     #predictors
model.lasso <- lars(x, y, type="lasso")
plot(model.lasso)
model.lasso$lambda
summary(model.lasso)
lambda.lasso <- c(model.lasso$lambda,0)
beta <- coef(model.lasso)
str(model.lasso)
colors <- rainbow(8)
matplot(lambda.lasso, beta, xlim=c(8,-2), type="o", pch=20, xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors)
text(rep(-0, 9), beta[9,], colnames(x), pos=4, col=colors)
abline(v=lambda.lasso[4], lty=2)
abline(h=0, lty=2)
beta.lasso <- beta[4,]
resid.lasso <- train$lpsa - predict(model.lasso, as.matrix(train[,1:8]), s=4, type="fit")$fit
rss.lasso <- sum(resid.lasso^2)/(67-4)
cvlas<-cv.lars(x,y,type="lasso",mode="fraction")
cvlas
opt.frac <- min(cvlas$cv) + sd(cvlas$cv)
opt.frac <- cvlas$index[which(cvlas$cv < opt.frac)[1]]
lasso.path <- lars(x, y, type = "lasso")
summary(lasso.path)
lasso.fit <- predict.lars(lasso.path, type = "coefficients", mode = "fraction", s = opt.frac)
coef(lasso.fit)
cvlas$index
opt.frac <- min(cvlas$cv) + sd(cvlas$cv)
match(opt.frac,cvlas$index)
opt.frac <- cvlas$index[which(cvlas$cv < opt.frac)[1]]
match(opt.frac,cvlas$index)
opt.frac <- min(cvlas$cv) + sd(cvlas$cv)
opt.frac <- cvlas$index[which(cvlas$cv < opt.frac)[1]]
match(opt.frac,cvlas$index)
opt.frac <- min(cvlas$cv) + sd(cvlas$cv)
cvlas
which(cvlas$cv < opt.frac
#or a function that might even be easier (from the glmnet package)
library(glmnet)
cvfit = cv.glmnet(x, y)
plot(cvfit)
cvfit
# #########################
# # PARTIAL LEAST SQUARES #
# #########################
#
# Partial Least Squares is pretty much like principal components regression, but
# we use information from Y to select weights for the principal components of X.
library(pls)
model.pls <- plsr(lpsa ~ ., 8, data = train, method = "oscorespls", validation = "CV")
summary(model.pls)
plot(model.pls, ncomp = 1, asp = 1, line = TRUE)
plot(model.pls, ncomp = 2, asp = 1, line = TRUE)
plot(model.pls, ncomp = 3, asp = 1, line = TRUE)
plot(model.pls, ncomp = 4, asp = 1, line = TRUE)
plot(model.pls, ncomp = 8, asp = 1, line = TRUE)
plot(RMSEP(model.pls),legendpos="topright")
# I'm eyeballing CV here, but fitting 4 components should be enough. So I'll update
# the model
model.pls <- plsr(lpsa ~ .,4, data = train, method = "oscorespls")
plot(model.pls, plottype = "scores", comps = 1:3)
beta.pls <- drop(coef(model.pls))
resid.pls <- drop(model.pls$resid)[,4]
rss.pls <- sum(resid.pls^2)/(67-4)
# #########################
# # COMPARISON OF FITTING #
# #########################
#
# This is as straightforward as it gets:
rss.ls
rss.backward
rss.forward
rss.ridge
rss.lasso
rss.pls
# ############################
# # COMPARISON OF PREDICTION #
# ############################
#
# We can also compare with the predicition dataset we saved from before. In this case
y.new <- calibrate$lpsa
pss.ls <- sum((y.new - predict(model.ls, calibrate[,1:8]))^2)
pss.backward <- sum((y.new - predict(model.backward, calibrate[,1:8]))^2)
pss.forward <- sum((y.new - predict(model.forward, calibrate[,1:8]))^2)
pss.ridge <- sum((y.new - beta.ridge[1] - as.matrix(calibrate[,1:8])%*%beta.ridge[2:9])^2)
pss.lasso <- sum((y.new - predict(model.lasso, as.matrix(calibrate[,1:8]), s=4, type="fit")$fit)^2)
pss.pls <- sum((y.new - drop(predict(model.pls, calibrate[,1:8], 4)))^2)
pss.ls
pss.backward
pss.forward
pss.ridge
pss.lasso
pss.pls
cvlas
opt.frac <- min(cvlas$cv) + sd(cvlas$cv)
library(caret)
data("GermanCredit")
str(GermanCredit)
summary(GermanCredit)
library(glm)
library(glmnet)
library(randomForest)
model <- randomForest(Class~.,data=GermanCredit,ntree=200,importance=TRUE)
fit<-predict(model,GermanCredit,type="class")
fit
mean(fit,GermanCredit$class)
mean(fit=GermanCredit$class)
mean(fit==GermanCredit$class)
predict<-predict(model,GermanCredit,type="class")
mean(predict==GermanCredit$class)
install.packages("ROCR")
library(ROCR)
binary_eval(predict,GermanCredit$class)
library(ROCR)
library(gplots)
install.packages("gtools")
library(gplots)
install.packages("caTools")
library(gplots)
library(ROCR)
library(ROCR)
binary_eval(predict,GermanCredit$class)
fit<-predict(model,GermanCredit,type="class")
rm("predict")
library(car)
influencePlot(fit)
fit <- glm(data=GermanCredit, Class ~ . , family="binomial")
summary(fit)
table(GermanCredit$Class)
library(car)
influencePlot(fit)
library(gmodels)
val <- runif(1000)
val <- fit$fitted.values
CrossTable(val>0.5, fit$y, chisq=T, prop.chisq = T)
library(gmodels)
install.packages("gmodels")
library(gmodels)
val <- runif(1000)
val <- fit$fitted.values
CrossTable(val>0.5, fit$y, chisq=T, prop.chisq = T)
confusionMatrix(as.factor(val>0.5), as.factor(fit$y==1), positive="TRUE")
library(tidyverse)
library(tidyverse)
install.packages("tidyverse")
library(VIM)
library(fiftystater)
library(fifystarter)
library(map)
library(maps)
install.packages("maps")
install.packages("fifitystater")
install.packages("fiftystater")
install.packages("fiftystater")
okmap = fifty_states[fifty_states$id=="oklahoma"]
library(fiftystater) # state map
library(RColorBrewer)
library(gridExtra)
install.packages("fiftystater")
install.packages("fiftystater")
R-- version
R --version
version
library(ggplot2)    #for graphics and the dataset for this example session
library(cluster)    #provides more cluster algorithms than base R (e.g. PAM)
library(useful)     #provides a plot function for clusters and "FitKMeans" and "PlotHartigan" functions
library(NbClust)    #provides tons of tools for identifying the "right" number of clusters
library(rgl)        #for 3D rotating plots
?kmeans
data (cars,package="caret")
cars<-cars[,c("Price","Mileage")]   #price and mileage for several cars listed for resale in 2005 Kelly Blue Book
carScaled<-data.frame(scale(cars))  #default is mean centering with scaling by standard deviation
carsKM <- kmeans(carScaled,3, nstart=10)
carsKM$centers  # the centroids of the final clusers (remember, these are scaled)
carsKM$size #and the size of each cluster
carsKM$size #and the size of each cluster
carsData$Cluster<-carsKM$cluster
p<-qplot(data=carsData, x=Price, y=Mileage, color=factor(carsKM$cluster))  #plot the 2 variables and the cluster color
g <- guide_legend("Cluster")                  #retitle the legend...
p + guides(color = g, size = g, shape = g)    #retitle the legend...
p<-qplot(data=carsData, x=Price, y=Mileage, color=factor(carsKM$cluster))  #plot the 2 variables and the cluster color
carsData$Cluster<-carsKM$cluster
carsKM$centers  # the centroids of the final clusers (remember, these are scaled)
carsKM$size #and the size of each cluster
carsData<-cars
carsData$Cluster<-carsKM$cluster
p<-qplot(data=carsData, x=Price, y=Mileage, color=factor(carsKM$cluster))  #plot the 2 variables and the cluster color
g <- guide_legend("Cluster")                  #retitle the legend...
p + guides(color = g, size = g, shape = g)    #retitle the legend...
p<-qplot(data=carsData, x=Price, y=Mileage, color=factor(carsKM$cluster))  #plot the 2 variables and the cluster color
p <- p+  geom_point(data=cars[c(23,576),], aes(x=Price, y=Mileage), size=2,colour="black")
g <- guide_legend("Cluster")                  #retitle the legend...
p + guides(color = g, size = g, shape = g)    #retitle the legend...
wssplot <- function(data, nc=15){
par(mfrow=c(1,2))
wss <- NULL
pctExp <-NULL
for (k in 1:nc)
{
kclus <- kmeans(data, centers=k)
wss[k] <- kclus$tot.withinss      #store the total within SSE for given k
pctExp[k] <- 1-wss[k]/kclus$totss
}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
plot(1:nc, pctExp, type="b", xlab="Number of Clusters",
ylab="Pct Explained")
par(mfrow=c(1,1))
}
wssplot(carScaled,nc=30)
clusFit<-FitKMeans(carScaled,max.clusters=30,nstart=20)   #evaluates k using the "Hartigan" rule
clusFit
PlotHartigan(clusFit)
carsKM <- kmeans(carScaled,19, nstart=10)
p<-qplot(data=cars, x=Price, y=Mileage, color=factor(carsKM$cluster))  #plot the 2 variables and the cluster color
g <- guide_legend("Cluster")                  #retitle the legend...
p + guides(color = g, size = g, shape = g)    #retitle the legend...
carSample <- carScaled[sample(1:nrow(carScaled),100),]
carSample <- carScaled[sample(1:nrow(carScaled),100),]
carScaled<-data.frame(scale(cars))
carsPAM <- pam(carScaled, 5)
carsKM <- kmeans(carScaled,5)
carsKM$centers   #and compare with centroids from KM
str(carsPAM)
PAM<- data.frame(carsPAM$medoids)
p<-qplot(data=carScaled, x=Price, y=Mileage, color=factor(carsPAM$clustering))  #plot the 2 variables and the cluster color
g <- guide_legend("Cluster")                  #retitle the legend...
p <- p+guides(color = g, size = g, shape = g)    #retitle the legend...
p <- p + geom_point(data=PAM, aes(x=Price, y=Mileage), size=5,colour="black")
p
KM<- data.frame(carsKM$centers)
p <- p+  geom_point(data=KM, aes(x=Price, y=Mileage), size=5,colour="blue")
p
carSample <- carScaled[sample(1:nrow(carScaled),100),]
carsPAMs <- pam(carSample, 5)
str(si <- silhouette(carsPAMs))
(ssi <- summary(si))
plot(si) # silhouette plot
(results<-NbClust(carSample,method="kmeans"))
p<-qplot(data=carSample, x=Price, y=Mileage, color=factor(results$Best.partition))  #plot the 2 variables and the cluster color
g <- guide_legend("Cluster")                  #retitle the legend...
p + guides(color = g, size = g, shape = g)    #retitle the legend...
p<-qplot(data=carSample, x=Price, y=Mileage, color=factor(results$Best.partition))  #plot the 2 variables and the cluster color
g <- guide_legend("Cluster")                  #retitle the legend...
p + guides(color = g, size = g, shape = g)    #retitle the legend...
set.seed(42)
data(iris)
iris$Species <- factor(iris$Species,
levels = c("versicolor","virginica","setosa"))
iris[,1:4]<-scale(iris[,1:4])  # first scale the data
head(iris)
di <- dist(iris[,1:4], method="euclidean")   # with hiearchical clustering, only need distance matrix
hc <- hclust(di, method="ward.D")
plot(hc, labels=FALSE)
rect.hclust(hc, k=2, border="red")     #if we were to "cut" at k=3, what are the groups?
iris$hcluster <- as.factor(cutree(hc, k=2))   #cutting at k=3, here are the assignments
head(iris)
table( iris$Species, iris$hcluster)
dev.off()
data(mtcars)
head(mtcars)
d<-daisy(mtcars,metric="manhattan",stand=T)
hclus<-hclust(d,method="single")   #notice the long chains (e.g., very unbalanced)
plot(hclus)
d<-daisy(mtcars,metric="euclidean",stand=T)
str(mtcars)
mtcars$vs <- factor(mtcars$vs)
mtcars$am <- factor(mtcars$am)
d<-daisy(mtcars,metric="gower",stand=T)
hclus<-hclust(d,method="single")   #notice the long chains (e.g., very unbalanced)
plot(hclus)
carScaled<-data.frame(scale(cars))
carsKM <- kmeans(carScaled,4, nstart=10)
p<-qplot(data=carScaled, x=Price, y=Mileage, color=factor(carsKM$cluster))  #plot the 2 variables and the cluster color
g <- guide_legend("Cluster")                  #retitle the legend...
p + guides(color = g, size = g, shape = g)    #retitle the legend...
d<-daisy(carScaled)
hclus<-hclust(d,method="average")
plot(hclus)
rect.hclust(hclus, k=4, border="red")     #if we were to "cut" at k=4, what are the groups?
carScaled$hcluster <- as.factor(cutree(hclus, k=4))   #cutting at k=4, here are the assignments
p<-qplot(data=carScaled, x=Price, y=Mileage, color=factor(carScaled$hcluster))  #plot the 2 variables and the cluster color
g <- guide_legend("Cluster")                  #retitle the legend...
p + guides(color = g, size = g, shape = g)    #retitle the legend...
data(movies)
View(movies)
movies2<-movies[complete.cases(movies),c(1:6,17)]    #keep only complete cases; keep some variables
movies2<-movies2[movies2$votes>2500,]                #exclude movies with only a few ratings
mDD<-duplicated(movies2[,c("title")])
movies2<-movies2[!mDD,]
dat<-round(scale(movies2[,-c(1,7)]),3)               #scale the numeric data (and round the results)
row.names(dat)<-movies2$title                        #keep the movie titles as row.names
dat<-as.data.frame(dat)
head(dat)
library(ggplot2movies)
data(movies)
View(movies)
movies2<-movies[complete.cases(movies),c(1:6,17)]    #keep only complete cases; keep some variables
movies2<-movies2[movies2$votes>2500,]                #exclude movies with only a few ratings
mDD<-duplicated(movies2[,c("title")])
movies2<-movies2[!mDD,]
library(ggplot2movies)
install.packages("ggplot2movies")
library(ggplot2movies)
data(movies)
View(movies)
movies2<-movies[complete.cases(movies),c(1:6,17)]    #keep only complete cases; keep some variables
movies2<-movies2[movies2$votes>2500,]                #exclude movies with only a few ratings
mDD<-duplicated(movies2[,c("title")])
movies2<-movies2[!mDD,]
dat<-round(scale(movies2[,-c(1,7)]),3)               #scale the numeric data (and round the results)
row.names(dat)<-movies2$title                        #keep the movie titles as row.names
dat<-as.data.frame(dat)
head(dat)
wssplot(dat,nc=30)   #-- again, no clear elbow!
set.seed(100)      #just so that we will all get the same results!
kclus<-kmeans(dat, 6, nstart=5)    #how about trying k=6
plot(kclus,data=dat) #this plot function comes from the "useful" libary uses PCA
clusInfo<-data.frame(kclus$centers,kclus$size)
clusInfo
movieClus <- data.frame(movies2, clust=kclus$cluster, dat)
head(movieClus[movieClus$clust==2,c("title","rating","budget","year","mpaa")])
closest.cluster <- function(x)
{
cluster.dist <- apply(kclus$centers, 1, function(y) sqrt(sum((x-y)^2)))
return(cluster.dist[which.min(cluster.dist)[1]])
}
movieClus$dist <- apply(dat, 1, closest.cluster)   #apply the "distance to nearest cluster function"
movieClus <-movieClus [order(movieClus$dist),]         #sort the data by this new distance
head(movieClus[movieClus$clust==1,c("title","rating","budget","year","dist")],10)
head(movieClus[movieClus$clust==2,c("title","rating","budget","year","dist")],10)
head(movieClus[movieClus$clust==3,c("title","rating","budget","year","dist")],10)
head(movieClus[movieClus$clust==4,c("title","rating","budget","year","dist")],10)
head(movieClus[movieClus$clust==5,c("title","rating","budget","year","dist")],10)
head(movieClus[movieClus$clust==6,c("title","rating","budget","year","dist")],10)
d<-daisy(dat)
hclus<-hclust(d,method="complete")
plot(hclus, labels=FALSE)
rect.hclust(hclus, k=6, border="red")     #if we were to "cut" at k=3, what are the groups?
pcaMovies<-prcomp(dat,scale=T)
dat$hcluster <- as.factor(cutree(hclus, k=6))   #cutting at k=3, here are the assignments
p<-qplot(data=data.frame(pcaMovies$x), x=PC1, y=PC2, color=factor(dat$hcluster))  #plot the 2 variables and the cluster color
g <- guide_legend("Cluster")                  #retitle the legend...
p + guides(color = g, size = g, shape = g)    #retitle the legend...
dat<-round(scale(movies2[,-c(1,7)]),3)               #scale the numeric data (and round the results)
row.names(dat)<-movies2$title                        #keep the movie titles as row.names
dat<-as.data.frame(dat)
wssplot(dat,nc=30)   #-- again, no clear elbow!
kclus<-kmeans(dat, 6, nstart=5)    #how about trying k=6
dat$hcluster <- as.factor(cutree(hclus, k=6))   #cutting at k=3, here are the assignments
p<-qplot(data=data.frame(pcaMovies$x), x=PC1, y=PC2, color=factor(dat$hcluster))  #plot the 2 variables and the cluster color
g <- guide_legend("Cluster")                  #retitle the legend...
p + guides(color = g, size = g, shape = g)    #retitle the legend...
library(Hmisc)
v<-varclus(as.matrix(dat),similarity="pearson")
plot(v)
library(Hmisc)
install.packages("Hmisc")
library(Hmisc)
v<-varclus(as.matrix(dat),similarity="pearson")
plot(v)
v<-varclus(as.matrix(dat),similarity="pearson")
install.packages("Hmisc")
library(Hmisc)
corMat<-cor(dat,method="pearson")
heatmap(corMat)
library(dbscan)
install.packages("dbscan")
data(iris)
library(dbscan)
data(iris)
iris2 <- as.matrix(iris[,1:4])
kNNdist(iris2, k=4, search="kd")
kNNdistplot(iris2, k=4)
abline(h=.5, col="red")
pairs(iris2, col = db$cluster+1L)
table(iris$Species, db$cluster)
kNNdist(iris2, k=4, search="kd")
kNNdistplot(iris2, k=4)
abline(h=.5, col="red")
abline(h=.5, col="red")
db <- dbscan(iris2, eps = .5, minPts = 4)
pairs(iris2, col = db$cluster+1L)
table(iris$Species, db$cluster)
x=dat[,1:5]
str(x)
?kNNdist
kNNdistplot(x, k=9)
abline(h=1.05, col="red")
set.seed(1234)
db = dbscan(x, .95,5)
db
hullplot(x, db$cluster)
table(db$cluster)
opt <- optics(x, eps = .51, minPts = 10)
opt
opt <- extractDBSCAN(opt, eps_cl = .51)
plot(opt)
library(dbscan)
set.seed(665544)
n <- 1000
x <- cbind(
x = runif(10, 0, 10) + rnorm(n, sd = 0.75),
y = runif(10, 0, 10) + rnorm(n, sd = 0.2)
)
p<-qplot(data=as.data.frame(x), x=x, y=y)
install.packages("qplot")
load("~/GitHub/AIPrediction/data/AIproduction.RData")
